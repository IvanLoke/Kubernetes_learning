
==> Audit <==
|---------|-----------------------------|----------|------|---------|---------------------|---------------------|
| Command |            Args             | Profile  | User | Version |     Start Time      |      End Time       |
|---------|-----------------------------|----------|------|---------|---------------------|---------------------|
| start   |                             | minikube | ivan | v1.33.1 | 03 Sep 24 20:47 +08 |                     |
| start   |                             | minikube | ivan | v1.33.1 | 03 Sep 24 20:54 +08 | 03 Sep 24 20:56 +08 |
| service | mongo-express               | minikube | ivan | v1.33.1 | 03 Sep 24 21:33 +08 |                     |
| service | mongo-express-service       | minikube | ivan | v1.33.1 | 03 Sep 24 21:33 +08 |                     |
| service | mongo-express-service       | minikube | ivan | v1.33.1 | 03 Sep 24 21:34 +08 |                     |
| service | mongo-express-service --url | minikube | ivan | v1.33.1 | 03 Sep 24 21:36 +08 | 03 Sep 24 21:42 +08 |
| service | mongo-express-service       | minikube | ivan | v1.33.1 | 03 Sep 24 21:42 +08 |                     |
|---------|-----------------------------|----------|------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/09/03 20:54:21
Running on machine: LAPTOP-RF7OBBU6
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0903 20:54:21.530744    2781 out.go:291] Setting OutFile to fd 1 ...
I0903 20:54:21.530858    2781 out.go:343] isatty.IsTerminal(1) = true
I0903 20:54:21.530861    2781 out.go:304] Setting ErrFile to fd 2...
I0903 20:54:21.530863    2781 out.go:343] isatty.IsTerminal(2) = true
I0903 20:54:21.530982    2781 root.go:338] Updating PATH: /home/ivan/.minikube/bin
W0903 20:54:21.531062    2781 root.go:314] Error reading config file at /home/ivan/.minikube/config/config.json: open /home/ivan/.minikube/config/config.json: no such file or directory
I0903 20:54:21.531182    2781 out.go:298] Setting JSON to false
I0903 20:54:21.531623    2781 start.go:129] hostinfo: {"hostname":"LAPTOP-RF7OBBU6","uptime":481,"bootTime":1725367581,"procs":43,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"ca73d109-59b1-466c-b6c7-f8e488524345"}
I0903 20:54:21.531655    2781 start.go:139] virtualization:  guest
I0903 20:54:21.537309    2781 out.go:177] 😄  minikube v1.33.1 on Ubuntu 22.04 (amd64)
W0903 20:54:21.538085    2781 preload.go:294] Failed to list preload files: open /home/ivan/.minikube/cache/preloaded-tarball: no such file or directory
I0903 20:54:21.538160    2781 notify.go:220] Checking for updates...
I0903 20:54:21.538214    2781 driver.go:392] Setting default libvirt URI to qemu:///system
I0903 20:54:21.538241    2781 global.go:112] Querying for installed drivers using PATH=/home/ivan/.minikube/bin:/home/ivan/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Program Files/WindowsApps/MicrosoftCorporationII.WindowsSubsystemForLinux_2.2.4.0_x64__8wekyb3d8bbwe:/mnt/c/Program Files/Eclipse Adoptium/jdk-17.0.5.8-hotspot/bin:/mnt/c/Program Files (x86)/Razer Chroma SDK/bin:/mnt/c/Program Files/Razer Chroma SDK/bin:/mnt/c/Program Files (x86)/Razer/ChromaBroadcast/bin:/mnt/c/Program Files/Razer/ChromaBroadcast/bin:/mnt/c/Program Files (x86)/Common Files/Oracle/Java/javapath:/mnt/c/ProgramData/Oracle/Java/javapath:/mnt/c/Windows/System32:/mnt/c/Windows:/mnt/c/Windows/System32/Wbem:/mnt/c/Windows/System32/WindowsPowerShell/v1.0/:/mnt/c/Windows/System32/OpenSSH/:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X64/:/mnt/c/Program Files (x86)/Pulse Secure/VC142.CRT/X86/:/mnt/c/Windows/system32/config/systemprofile/AppData/Local/Microsoft/WindowsApps:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files/nodejs/:/mnt/c/Windows/System32/WindowsPowerShell/v1.0:/mnt/c/flutter/bin:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/Git/mingw64/bin:/mnt/c/Program Files/Git/usr/bin:/mnt/c/Program Files (x86)/Microsoft SQL Server/160/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/160/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/Client SDK/ODBC/170/Tools/Binn/:/mnt/c/Program Files/Microsoft SQL Server/160/DTS/Binn/:/mnt/c/Program Files (x86)/Microsoft SQL Server/160/DTS/Binn/:/mnt/c/Program Files/Azure Data Studio/bin:/mnt/c/Users/Ivan/AppData/Local/Programs/Python/Python310/Scripts:/mnt/c/Program Files/Docker/Docker/resources/bin:/mnt/c/Users/Ivan/AppData/Local/Programs/Python/Python310/Scripts/:/mnt/c/Users/Ivan/AppData/Local/Programs/Python/Python310/:/mnt/c/Users/Ivan/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/Ivan/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Program Files/JetBrains/PyCharm Community Edition 2021.3.1/bin:/mnt/c/Users/Ivan/AppData/Local/GitHubDesktop/bin:/mnt/c/MinGW/bin:/mnt/c/Users/Ivan/.dotnet/tools:/mnt/c/Users/Ivan/AppData/Roaming/npm:/mnt/c/Program Files/Git/bin/git.exe:/mnt/c/Program Files/JetBrains/IntelliJ IDEA Community Edition 2023.2.2/bin:/snap/bin
I0903 20:54:21.592284    2781 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I0903 20:54:21.640594    2781 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0903 20:54:21.742890    2781 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0903 20:54:21.793216    2781 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0903 20:54:21.815170    2781 docker.go:122] docker version: linux-27.1.1:Docker Desktop  ()
I0903 20:54:21.815246    2781 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0903 20:54:22.097780    2781 info.go:266] docker info: {ID:b56d3cc9-06e7-4172-8731-d5a8c3c4d2b7 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:70 SystemTime:2024-09-03 12:54:22.088248987 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8222965760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0903 20:54:22.097878    2781 docker.go:295] overlay module found
I0903 20:54:22.097905    2781 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0903 20:54:22.103309    2781 global.go:133] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0903 20:54:22.153758    2781 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0903 20:54:22.153798    2781 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0903 20:54:22.153817    2781 driver.go:314] not recommending "none" due to default: false
I0903 20:54:22.153820    2781 driver.go:314] not recommending "ssh" due to default: false
I0903 20:54:22.153830    2781 driver.go:349] Picked: docker
I0903 20:54:22.153834    2781 driver.go:350] Alternatives: [none ssh]
I0903 20:54:22.153838    2781 driver.go:351] Rejects: [kvm2 qemu2 virtualbox vmware podman]
I0903 20:54:22.155447    2781 out.go:177] ✨  Automatically selected the docker driver. Other choices: none, ssh
I0903 20:54:22.156095    2781 start.go:297] selected driver: docker
I0903 20:54:22.156100    2781 start.go:901] validating driver "docker" against <nil>
I0903 20:54:22.156108    2781 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0903 20:54:22.156167    2781 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0903 20:54:22.443686    2781 info.go:266] docker info: {ID:b56d3cc9-06e7-4172-8731-d5a8c3c4d2b7 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:51 OomKillDisable:true NGoroutines:70 SystemTime:2024-09-03 12:54:22.435102759 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8222965760 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0903 20:54:22.443819    2781 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0903 20:54:22.444114    2781 start_flags.go:393] Using suggested 2200MB memory alloc based on sys=7842MB, container=7842MB
I0903 20:54:22.444240    2781 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0903 20:54:22.445304    2781 out.go:177] 📌  Using Docker driver with root privileges
W0903 20:54:22.446056    2781 out.go:239] ❗  For an improved experience it's recommended to use Docker Engine instead of Docker Desktop.
Docker Engine installation instructions: https://docs.docker.com/engine/install/#server
I0903 20:54:22.446106    2781 cni.go:84] Creating CNI manager for ""
I0903 20:54:22.446116    2781 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0903 20:54:22.446122    2781 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0903 20:54:22.446184    2781 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ivan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0903 20:54:22.447512    2781 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0903 20:54:22.448315    2781 cache.go:121] Beginning downloading kic base image for docker with docker
I0903 20:54:22.449057    2781 out.go:177] 🚜  Pulling base image v0.0.44 ...
I0903 20:54:22.449692    2781 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0903 20:54:22.449788    2781 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local docker daemon
I0903 20:54:22.466326    2781 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0903 20:54:22.466473    2781 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e in local cache directory
I0903 20:54:22.466706    2781 image.go:118] Writing gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e to local cache
I0903 20:54:22.755861    2781 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0903 20:54:22.755881    2781 cache.go:56] Caching tarball of preloaded images
I0903 20:54:22.756172    2781 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0903 20:54:22.759716    2781 out.go:177] 💾  Downloading Kubernetes v1.30.0 preload ...
I0903 20:54:22.762137    2781 preload.go:237] getting checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0903 20:54:22.965414    2781 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.30.0/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4?checksum=md5:00b6acf85a82438f3897c0a6fafdcee7 -> /home/ivan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0903 20:55:29.089719    2781 preload.go:248] saving checksum for preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0903 20:55:29.089774    2781 preload.go:255] verifying checksum of /home/ivan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 ...
I0903 20:55:29.705934    2781 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0903 20:55:29.706165    2781 profile.go:143] Saving config to /home/ivan/.minikube/profiles/minikube/config.json ...
I0903 20:55:29.706184    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/config.json: {Name:mkbb5bb22ebbf88b406caaf10cad731618db549b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:55:44.240189    2781 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e as a tarball
I0903 20:55:44.240200    2781 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from local cache
I0903 20:55:55.978498    2781 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e from cached tarball
I0903 20:55:55.978561    2781 cache.go:194] Successfully downloaded all kic artifacts
I0903 20:55:55.979333    2781 start.go:360] acquireMachinesLock for minikube: {Name:mk022bab7e43759416bba16bc6077070df57db3c Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0903 20:55:55.979551    2781 start.go:364] duration metric: took 189.395µs to acquireMachinesLock for "minikube"
I0903 20:55:55.979929    2781 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ivan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0903 20:55:55.980022    2781 start.go:125] createHost starting for "" (driver="docker")
I0903 20:55:55.982542    2781 out.go:204] 🔥  Creating docker container (CPUs=2, Memory=2200MB) ...
I0903 20:55:55.984358    2781 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0903 20:55:55.984422    2781 client.go:168] LocalClient.Create starting
I0903 20:55:55.985221    2781 main.go:141] libmachine: Creating CA: /home/ivan/.minikube/certs/ca.pem
I0903 20:55:56.110785    2781 main.go:141] libmachine: Creating client certificate: /home/ivan/.minikube/certs/cert.pem
I0903 20:55:56.245111    2781 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0903 20:55:56.269483    2781 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0903 20:55:56.269590    2781 network_create.go:281] running [docker network inspect minikube] to gather additional debugging logs...
I0903 20:55:56.269606    2781 cli_runner.go:164] Run: docker network inspect minikube
W0903 20:55:56.290925    2781 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0903 20:55:56.290948    2781 network_create.go:284] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0903 20:55:56.290957    2781 network_create.go:286] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0903 20:55:56.291426    2781 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0903 20:55:56.313476    2781 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001b21110}
I0903 20:55:56.313680    2781 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0903 20:55:56.313734    2781 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0903 20:55:56.375485    2781 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0903 20:55:56.375555    2781 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0903 20:55:56.375653    2781 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0903 20:55:56.396976    2781 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0903 20:55:56.417974    2781 oci.go:103] Successfully created a docker volume minikube
I0903 20:55:56.418067    2781 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib
I0903 20:55:58.243880    2781 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -d /var/lib: (1.82577503s)
I0903 20:55:58.243906    2781 oci.go:107] Successfully prepared a docker volume minikube
I0903 20:55:58.243965    2781 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0903 20:55:58.244197    2781 kic.go:194] Starting extracting preloaded images to volume ...
I0903 20:55:58.244280    2781 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/ivan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir
I0903 20:56:06.913400    2781 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/ivan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e -I lz4 -xf /preloaded.tar -C /extractDir: (8.669027945s)
I0903 20:56:06.913430    2781 kic.go:203] duration metric: took 8.669440193s to extract preloaded images to volume ...
W0903 20:56:06.914900    2781 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
I0903 20:56:06.917983    2781 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0903 20:56:07.939881    2781 cli_runner.go:217] Completed: docker info --format "'{{json .SecurityOptions}}'": (1.02277447s)
I0903 20:56:07.940065    2781 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e
I0903 20:56:08.612043    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0903 20:56:08.715207    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:08.772889    2781 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0903 20:56:09.090152    2781 oci.go:144] the created container "minikube" has a running status.
I0903 20:56:09.091849    2781 kic.go:225] Creating ssh key for kic: /home/ivan/.minikube/machines/minikube/id_rsa...
I0903 20:56:09.276254    2781 kic_runner.go:191] docker (temp): /home/ivan/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0903 20:56:09.402457    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:09.447856    2781 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0903 20:56:09.447873    2781 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0903 20:56:09.597934    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:09.675810    2781 machine.go:94] provisionDockerMachine start ...
I0903 20:56:09.677557    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:09.748458    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:09.749401    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:09.749495    2781 main.go:141] libmachine: About to run SSH command:
hostname
I0903 20:56:09.757070    2781 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0903 20:56:13.070741    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0903 20:56:13.073157    2781 ubuntu.go:169] provisioning hostname "minikube"
I0903 20:56:13.074112    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:13.113883    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:13.114045    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:13.114053    2781 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0903 20:56:13.326639    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0903 20:56:13.327196    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:13.367310    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:13.367530    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:13.367548    2781 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0903 20:56:13.544314    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0903 20:56:13.546683    2781 ubuntu.go:175] set auth options {CertDir:/home/ivan/.minikube CaCertPath:/home/ivan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/ivan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/ivan/.minikube/machines/server.pem ServerKeyPath:/home/ivan/.minikube/machines/server-key.pem ClientKeyPath:/home/ivan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/ivan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/ivan/.minikube}
I0903 20:56:13.546718    2781 ubuntu.go:177] setting up certificates
I0903 20:56:13.547531    2781 provision.go:84] configureAuth start
I0903 20:56:13.547630    2781 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0903 20:56:13.581203    2781 provision.go:143] copyHostCerts
I0903 20:56:13.581674    2781 exec_runner.go:151] cp: /home/ivan/.minikube/certs/cert.pem --> /home/ivan/.minikube/cert.pem (1115 bytes)
I0903 20:56:13.581861    2781 exec_runner.go:151] cp: /home/ivan/.minikube/certs/key.pem --> /home/ivan/.minikube/key.pem (1679 bytes)
I0903 20:56:13.581955    2781 exec_runner.go:151] cp: /home/ivan/.minikube/certs/ca.pem --> /home/ivan/.minikube/ca.pem (1074 bytes)
I0903 20:56:13.582750    2781 provision.go:117] generating server cert: /home/ivan/.minikube/machines/server.pem ca-key=/home/ivan/.minikube/certs/ca.pem private-key=/home/ivan/.minikube/certs/ca-key.pem org=ivan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0903 20:56:13.661886    2781 provision.go:177] copyRemoteCerts
I0903 20:56:13.662110    2781 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0903 20:56:13.662158    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:13.693953    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:13.799011    2781 ssh_runner.go:362] scp /home/ivan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0903 20:56:13.836142    2781 ssh_runner.go:362] scp /home/ivan/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0903 20:56:13.867503    2781 ssh_runner.go:362] scp /home/ivan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0903 20:56:13.897811    2781 provision.go:87] duration metric: took 349.938198ms to configureAuth
I0903 20:56:13.897836    2781 ubuntu.go:193] setting minikube options for container-runtime
I0903 20:56:13.898941    2781 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0903 20:56:13.899032    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:13.929777    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:13.929984    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:13.929991    2781 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0903 20:56:14.082414    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0903 20:56:14.082430    2781 ubuntu.go:71] root file system type: overlay
I0903 20:56:14.082537    2781 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0903 20:56:14.082598    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:14.108116    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:14.108282    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:14.108358    2781 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0903 20:56:14.263798    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0903 20:56:14.264065    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:14.295369    2781 main.go:141] libmachine: Using SSH client type: native
I0903 20:56:14.295582    2781 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d6e0] 0x830440 <nil>  [] 0s} 127.0.0.1 18508 <nil> <nil>}
I0903 20:56:14.295600    2781 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0903 20:56:15.413283    2781 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-04-30 11:46:26.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-09-03 12:56:14.247498826 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0903 20:56:15.413302    2781 machine.go:97] duration metric: took 5.737476605s to provisionDockerMachine
I0903 20:56:15.413311    2781 client.go:171] duration metric: took 19.429796513s to LocalClient.Create
I0903 20:56:15.413339    2781 start.go:167] duration metric: took 19.429896225s to libmachine.API.Create "minikube"
I0903 20:56:15.413607    2781 start.go:293] postStartSetup for "minikube" (driver="docker")
I0903 20:56:15.413617    2781 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0903 20:56:15.413661    2781 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0903 20:56:15.413693    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:15.434577    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:15.546429    2781 ssh_runner.go:195] Run: cat /etc/os-release
I0903 20:56:15.550165    2781 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0903 20:56:15.550193    2781 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0903 20:56:15.550202    2781 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0903 20:56:15.550209    2781 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0903 20:56:15.550503    2781 filesync.go:126] Scanning /home/ivan/.minikube/addons for local assets ...
I0903 20:56:15.551322    2781 filesync.go:126] Scanning /home/ivan/.minikube/files for local assets ...
I0903 20:56:15.551675    2781 start.go:296] duration metric: took 138.044819ms for postStartSetup
I0903 20:56:15.552087    2781 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0903 20:56:15.576674    2781 profile.go:143] Saving config to /home/ivan/.minikube/profiles/minikube/config.json ...
I0903 20:56:15.576903    2781 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0903 20:56:15.576938    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:15.607090    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:15.701896    2781 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0903 20:56:15.706361    2781 start.go:128] duration metric: took 19.727195645s to createHost
I0903 20:56:15.706402    2781 start.go:83] releasing machines lock for "minikube", held for 19.7277528s
I0903 20:56:15.706463    2781 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0903 20:56:15.747716    2781 ssh_runner.go:195] Run: cat /version.json
I0903 20:56:15.747781    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:15.747799    2781 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0903 20:56:15.748410    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:15.777268    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:15.786522    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:16.084340    2781 ssh_runner.go:195] Run: systemctl --version
I0903 20:56:16.088868    2781 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0903 20:56:16.095453    2781 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0903 20:56:16.122179    2781 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0903 20:56:16.122248    2781 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0903 20:56:16.148309    2781 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I0903 20:56:16.148377    2781 start.go:494] detecting cgroup driver to use...
I0903 20:56:16.148413    2781 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0903 20:56:16.149978    2781 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0903 20:56:16.165930    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0903 20:56:16.175624    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0903 20:56:16.187068    2781 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0903 20:56:16.187119    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0903 20:56:16.196951    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0903 20:56:16.206519    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0903 20:56:16.215632    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0903 20:56:16.224915    2781 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0903 20:56:16.233875    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0903 20:56:16.242573    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0903 20:56:16.251291    2781 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0903 20:56:16.260209    2781 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0903 20:56:16.268419    2781 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0903 20:56:16.275947    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:16.364219    2781 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0903 20:56:16.482574    2781 start.go:494] detecting cgroup driver to use...
I0903 20:56:16.482610    2781 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0903 20:56:16.482983    2781 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0903 20:56:16.494919    2781 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0903 20:56:16.494978    2781 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0903 20:56:16.506215    2781 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0903 20:56:16.522111    2781 ssh_runner.go:195] Run: which cri-dockerd
I0903 20:56:16.526330    2781 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0903 20:56:16.542628    2781 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0903 20:56:16.566919    2781 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0903 20:56:16.676503    2781 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0903 20:56:16.745900    2781 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0903 20:56:16.746858    2781 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0903 20:56:16.763587    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:16.875274    2781 ssh_runner.go:195] Run: sudo systemctl restart docker
I0903 20:56:17.314486    2781 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0903 20:56:17.326333    2781 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0903 20:56:17.337888    2781 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0903 20:56:17.449830    2781 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0903 20:56:17.549126    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:17.646810    2781 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0903 20:56:17.658594    2781 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0903 20:56:17.669471    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:17.785743    2781 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0903 20:56:17.857543    2781 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0903 20:56:17.858002    2781 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0903 20:56:17.863084    2781 start.go:562] Will wait 60s for crictl version
I0903 20:56:17.863137    2781 ssh_runner.go:195] Run: which crictl
I0903 20:56:17.869031    2781 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0903 20:56:18.020822    2781 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.1.1
RuntimeApiVersion:  v1
I0903 20:56:18.020892    2781 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0903 20:56:18.196441    2781 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0903 20:56:18.239872    2781 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.1.1 ...
I0903 20:56:18.240108    2781 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0903 20:56:18.259768    2781 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0903 20:56:18.264651    2781 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0903 20:56:18.279994    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0903 20:56:18.321795    2781 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ivan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0903 20:56:18.321920    2781 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0903 20:56:18.321983    2781 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0903 20:56:18.349040    2781 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0903 20:56:18.349185    2781 docker.go:615] Images already preloaded, skipping extraction
I0903 20:56:18.349482    2781 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0903 20:56:18.371311    2781 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0903 20:56:18.371729    2781 cache_images.go:84] Images are preloaded, skipping loading
I0903 20:56:18.371767    2781 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0903 20:56:18.372280    2781 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0903 20:56:18.372372    2781 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0903 20:56:18.757086    2781 cni.go:84] Creating CNI manager for ""
I0903 20:56:18.757112    2781 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0903 20:56:18.757162    2781 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0903 20:56:18.757203    2781 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0903 20:56:18.757367    2781 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0903 20:56:18.757483    2781 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0903 20:56:18.775596    2781 binaries.go:44] Found k8s binaries, skipping transfer
I0903 20:56:18.775655    2781 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0903 20:56:18.788889    2781 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0903 20:56:18.815665    2781 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0903 20:56:18.857662    2781 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0903 20:56:18.884849    2781 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0903 20:56:18.889713    2781 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0903 20:56:18.903627    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:19.107224    2781 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0903 20:56:19.130794    2781 certs.go:68] Setting up /home/ivan/.minikube/profiles/minikube for IP: 192.168.49.2
I0903 20:56:19.130832    2781 certs.go:194] generating shared ca certs ...
I0903 20:56:19.130881    2781 certs.go:226] acquiring lock for ca certs: {Name:mkac2643f8eccf353d546406f07bfc2c1bd1c392 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.131145    2781 certs.go:240] generating "minikubeCA" ca cert: /home/ivan/.minikube/ca.key
I0903 20:56:19.364324    2781 crypto.go:156] Writing cert to /home/ivan/.minikube/ca.crt ...
I0903 20:56:19.364352    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/ca.crt: {Name:mke2817c0609a9aa350c7f28f36c06c7b55e62dc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.364981    2781 crypto.go:164] Writing key to /home/ivan/.minikube/ca.key ...
I0903 20:56:19.364994    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/ca.key: {Name:mk2083eae7088da42715e015083af8de88e708e1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.365186    2781 certs.go:240] generating "proxyClientCA" ca cert: /home/ivan/.minikube/proxy-client-ca.key
I0903 20:56:19.500069    2781 crypto.go:156] Writing cert to /home/ivan/.minikube/proxy-client-ca.crt ...
I0903 20:56:19.500083    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/proxy-client-ca.crt: {Name:mk8e9f1ab6e18e6b7a9c488078505d53c467648c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.501368    2781 crypto.go:164] Writing key to /home/ivan/.minikube/proxy-client-ca.key ...
I0903 20:56:19.501382    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/proxy-client-ca.key: {Name:mkfaf5d4b7b4b685fedccbb6175d3c1b063921e7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.501667    2781 certs.go:256] generating profile certs ...
I0903 20:56:19.501760    2781 certs.go:363] generating signed profile cert for "minikube-user": /home/ivan/.minikube/profiles/minikube/client.key
I0903 20:56:19.501968    2781 crypto.go:68] Generating cert /home/ivan/.minikube/profiles/minikube/client.crt with IP's: []
I0903 20:56:19.698654    2781 crypto.go:156] Writing cert to /home/ivan/.minikube/profiles/minikube/client.crt ...
I0903 20:56:19.698668    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/client.crt: {Name:mkd3f57aaa29770ff0d47829eb9bae25e4193947 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.698910    2781 crypto.go:164] Writing key to /home/ivan/.minikube/profiles/minikube/client.key ...
I0903 20:56:19.698917    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/client.key: {Name:mkc9f55c1861f46890f9435dfdc8cef369edec1f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.699023    2781 certs.go:363] generating signed profile cert for "minikube": /home/ivan/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0903 20:56:19.699036    2781 crypto.go:68] Generating cert /home/ivan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0903 20:56:19.974977    2781 crypto.go:156] Writing cert to /home/ivan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0903 20:56:19.974991    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkdfca22298df1126d4ede44a2e2e26b7605b574 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.975172    2781 crypto.go:164] Writing key to /home/ivan/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0903 20:56:19.975178    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk19b04e60cce13a8c1c4ba835f8ac039ca42f49 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:19.975282    2781 certs.go:381] copying /home/ivan/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/ivan/.minikube/profiles/minikube/apiserver.crt
I0903 20:56:19.975335    2781 certs.go:385] copying /home/ivan/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/ivan/.minikube/profiles/minikube/apiserver.key
I0903 20:56:19.975367    2781 certs.go:363] generating signed profile cert for "aggregator": /home/ivan/.minikube/profiles/minikube/proxy-client.key
I0903 20:56:19.975377    2781 crypto.go:68] Generating cert /home/ivan/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0903 20:56:20.317975    2781 crypto.go:156] Writing cert to /home/ivan/.minikube/profiles/minikube/proxy-client.crt ...
I0903 20:56:20.317991    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/proxy-client.crt: {Name:mk2c8eb7c430caedcd6c00471e50a9c94b5084a0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:20.318215    2781 crypto.go:164] Writing key to /home/ivan/.minikube/profiles/minikube/proxy-client.key ...
I0903 20:56:20.318229    2781 lock.go:35] WriteFile acquiring /home/ivan/.minikube/profiles/minikube/proxy-client.key: {Name:mka49236d0b0cf45547fa2115c9d3683ab1b2a52 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:20.318583    2781 certs.go:484] found cert: /home/ivan/.minikube/certs/ca-key.pem (1679 bytes)
I0903 20:56:20.318667    2781 certs.go:484] found cert: /home/ivan/.minikube/certs/ca.pem (1074 bytes)
I0903 20:56:20.318690    2781 certs.go:484] found cert: /home/ivan/.minikube/certs/cert.pem (1115 bytes)
I0903 20:56:20.318707    2781 certs.go:484] found cert: /home/ivan/.minikube/certs/key.pem (1679 bytes)
I0903 20:56:20.322528    2781 ssh_runner.go:362] scp /home/ivan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0903 20:56:20.346937    2781 ssh_runner.go:362] scp /home/ivan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0903 20:56:20.369288    2781 ssh_runner.go:362] scp /home/ivan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0903 20:56:20.391435    2781 ssh_runner.go:362] scp /home/ivan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0903 20:56:20.414343    2781 ssh_runner.go:362] scp /home/ivan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0903 20:56:20.441762    2781 ssh_runner.go:362] scp /home/ivan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0903 20:56:20.469506    2781 ssh_runner.go:362] scp /home/ivan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0903 20:56:20.502455    2781 ssh_runner.go:362] scp /home/ivan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0903 20:56:20.541176    2781 ssh_runner.go:362] scp /home/ivan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0903 20:56:20.582023    2781 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0903 20:56:20.609867    2781 ssh_runner.go:195] Run: openssl version
I0903 20:56:20.650489    2781 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0903 20:56:20.672655    2781 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0903 20:56:20.680148    2781 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Sep  3 12:56 /usr/share/ca-certificates/minikubeCA.pem
I0903 20:56:20.680200    2781 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0903 20:56:20.693052    2781 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0903 20:56:20.702802    2781 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0903 20:56:20.707028    2781 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0903 20:56:20.707455    2781 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.44@sha256:eb04641328b06c5c4a14f4348470e1046bbcf9c2cbc551486e343d3a49db557e Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/ivan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0903 20:56:20.707562    2781 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0903 20:56:20.747292    2781 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0903 20:56:20.761340    2781 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0903 20:56:20.776957    2781 kubeadm.go:213] ignoring SystemVerification for kubeadm because of docker driver
I0903 20:56:20.777043    2781 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0903 20:56:20.792778    2781 kubeadm.go:154] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0903 20:56:20.792789    2781 kubeadm.go:156] found existing configuration files:

I0903 20:56:20.792840    2781 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0903 20:56:20.805640    2781 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0903 20:56:20.805696    2781 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0903 20:56:20.817405    2781 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0903 20:56:20.827368    2781 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0903 20:56:20.827427    2781 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0903 20:56:20.837248    2781 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0903 20:56:20.846979    2781 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0903 20:56:20.847018    2781 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0903 20:56:20.855937    2781 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0903 20:56:20.865590    2781 kubeadm.go:162] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0903 20:56:20.865641    2781 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0903 20:56:20.874710    2781 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.30.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0903 20:56:20.919140    2781 kubeadm.go:309] [init] Using Kubernetes version: v1.30.0
I0903 20:56:20.919183    2781 kubeadm.go:309] [preflight] Running pre-flight checks
I0903 20:56:21.059522    2781 kubeadm.go:309] [preflight] Pulling images required for setting up a Kubernetes cluster
I0903 20:56:21.059627    2781 kubeadm.go:309] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0903 20:56:21.059739    2781 kubeadm.go:309] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0903 20:56:21.371108    2781 kubeadm.go:309] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0903 20:56:21.373344    2781 out.go:204]     ▪ Generating certificates and keys ...
I0903 20:56:21.373516    2781 kubeadm.go:309] [certs] Using existing ca certificate authority
I0903 20:56:21.373604    2781 kubeadm.go:309] [certs] Using existing apiserver certificate and key on disk
I0903 20:56:21.471241    2781 kubeadm.go:309] [certs] Generating "apiserver-kubelet-client" certificate and key
I0903 20:56:21.562246    2781 kubeadm.go:309] [certs] Generating "front-proxy-ca" certificate and key
I0903 20:56:21.770992    2781 kubeadm.go:309] [certs] Generating "front-proxy-client" certificate and key
I0903 20:56:21.939425    2781 kubeadm.go:309] [certs] Generating "etcd/ca" certificate and key
I0903 20:56:22.052797    2781 kubeadm.go:309] [certs] Generating "etcd/server" certificate and key
I0903 20:56:22.053420    2781 kubeadm.go:309] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0903 20:56:22.481428    2781 kubeadm.go:309] [certs] Generating "etcd/peer" certificate and key
I0903 20:56:22.481948    2781 kubeadm.go:309] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0903 20:56:22.586730    2781 kubeadm.go:309] [certs] Generating "etcd/healthcheck-client" certificate and key
I0903 20:56:22.712554    2781 kubeadm.go:309] [certs] Generating "apiserver-etcd-client" certificate and key
I0903 20:56:22.791093    2781 kubeadm.go:309] [certs] Generating "sa" key and public key
I0903 20:56:22.791523    2781 kubeadm.go:309] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0903 20:56:22.949296    2781 kubeadm.go:309] [kubeconfig] Writing "admin.conf" kubeconfig file
I0903 20:56:23.342468    2781 kubeadm.go:309] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0903 20:56:23.499547    2781 kubeadm.go:309] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0903 20:56:23.739481    2781 kubeadm.go:309] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0903 20:56:23.862153    2781 kubeadm.go:309] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0903 20:56:23.863298    2781 kubeadm.go:309] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0903 20:56:23.868031    2781 kubeadm.go:309] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0903 20:56:23.871713    2781 out.go:204]     ▪ Booting up control plane ...
I0903 20:56:23.871991    2781 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0903 20:56:23.872175    2781 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0903 20:56:23.872288    2781 kubeadm.go:309] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0903 20:56:23.889191    2781 kubeadm.go:309] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0903 20:56:23.890700    2781 kubeadm.go:309] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0903 20:56:23.890755    2781 kubeadm.go:309] [kubelet-start] Starting the kubelet
I0903 20:56:24.017441    2781 kubeadm.go:309] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0903 20:56:24.017560    2781 kubeadm.go:309] [kubelet-check] Waiting for a healthy kubelet. This can take up to 4m0s
I0903 20:56:24.519715    2781 kubeadm.go:309] [kubelet-check] The kubelet is healthy after 502.330302ms
I0903 20:56:24.519888    2781 kubeadm.go:309] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0903 20:56:36.530511    2781 kubeadm.go:309] [api-check] The API server is healthy after 12.003858928s
I0903 20:56:36.621431    2781 kubeadm.go:309] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0903 20:56:36.679185    2781 kubeadm.go:309] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0903 20:56:36.928563    2781 kubeadm.go:309] [upload-certs] Skipping phase. Please see --upload-certs
I0903 20:56:36.933328    2781 kubeadm.go:309] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0903 20:56:36.961923    2781 kubeadm.go:309] [bootstrap-token] Using token: xnpubd.vym7v82zibbssxet
I0903 20:56:36.970111    2781 out.go:204]     ▪ Configuring RBAC rules ...
I0903 20:56:36.970442    2781 kubeadm.go:309] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0903 20:56:36.975126    2781 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0903 20:56:37.086846    2781 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0903 20:56:37.094812    2781 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0903 20:56:37.101510    2781 kubeadm.go:309] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0903 20:56:37.114158    2781 kubeadm.go:309] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0903 20:56:37.145539    2781 kubeadm.go:309] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0903 20:56:37.988244    2781 kubeadm.go:309] [addons] Applied essential addon: CoreDNS
I0903 20:56:38.068882    2781 kubeadm.go:309] [addons] Applied essential addon: kube-proxy
I0903 20:56:38.085252    2781 kubeadm.go:309] 
I0903 20:56:38.085421    2781 kubeadm.go:309] Your Kubernetes control-plane has initialized successfully!
I0903 20:56:38.085428    2781 kubeadm.go:309] 
I0903 20:56:38.085540    2781 kubeadm.go:309] To start using your cluster, you need to run the following as a regular user:
I0903 20:56:38.085545    2781 kubeadm.go:309] 
I0903 20:56:38.085578    2781 kubeadm.go:309]   mkdir -p $HOME/.kube
I0903 20:56:38.086198    2781 kubeadm.go:309]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0903 20:56:38.086295    2781 kubeadm.go:309]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0903 20:56:38.086302    2781 kubeadm.go:309] 
I0903 20:56:38.086442    2781 kubeadm.go:309] Alternatively, if you are the root user, you can run:
I0903 20:56:38.086448    2781 kubeadm.go:309] 
I0903 20:56:38.086519    2781 kubeadm.go:309]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0903 20:56:38.086524    2781 kubeadm.go:309] 
I0903 20:56:38.086601    2781 kubeadm.go:309] You should now deploy a pod network to the cluster.
I0903 20:56:38.086713    2781 kubeadm.go:309] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0903 20:56:38.086839    2781 kubeadm.go:309]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0903 20:56:38.086845    2781 kubeadm.go:309] 
I0903 20:56:38.087046    2781 kubeadm.go:309] You can now join any number of control-plane nodes by copying certificate authorities
I0903 20:56:38.087170    2781 kubeadm.go:309] and service account keys on each node and then running the following as root:
I0903 20:56:38.087176    2781 kubeadm.go:309] 
I0903 20:56:38.087379    2781 kubeadm.go:309]   kubeadm join control-plane.minikube.internal:8443 --token xnpubd.vym7v82zibbssxet \
I0903 20:56:38.087558    2781 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:5828609e50b0d2d9393fd37edde17bc23f1e2aa79c014b84b043ebeb8b4d7b4e \
I0903 20:56:38.087592    2781 kubeadm.go:309] 	--control-plane 
I0903 20:56:38.087598    2781 kubeadm.go:309] 
I0903 20:56:38.087749    2781 kubeadm.go:309] Then you can join any number of worker nodes by running the following on each as root:
I0903 20:56:38.087758    2781 kubeadm.go:309] 
I0903 20:56:38.087866    2781 kubeadm.go:309] kubeadm join control-plane.minikube.internal:8443 --token xnpubd.vym7v82zibbssxet \
I0903 20:56:38.088009    2781 kubeadm.go:309] 	--discovery-token-ca-cert-hash sha256:5828609e50b0d2d9393fd37edde17bc23f1e2aa79c014b84b043ebeb8b4d7b4e 
I0903 20:56:38.098698    2781 kubeadm.go:309] 	[WARNING Swap]: swap is supported for cgroup v2 only; the NodeSwap feature gate of the kubelet is beta but disabled by default
I0903 20:56:38.098852    2781 kubeadm.go:309] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0903 20:56:38.098941    2781 cni.go:84] Creating CNI manager for ""
I0903 20:56:38.099359    2781 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0903 20:56:38.102482    2781 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0903 20:56:38.107942    2781 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0903 20:56:38.149507    2781 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0903 20:56:38.212965    2781 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0903 20:56:38.213089    2781 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0903 20:56:38.213654    2781 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_03T20_56_38_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0903 20:56:39.928258    2781 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2024_09_03T20_56_38_0700 minikube.k8s.io/version=v1.33.1 minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff minikube.k8s.io/name=minikube minikube.k8s.io/primary=true: (1.714570097s)
I0903 20:56:39.928393    2781 ssh_runner.go:235] Completed: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj": (1.715402896s)
I0903 20:56:39.928410    2781 ops.go:34] apiserver oom_adj: -16
I0903 20:56:39.928523    2781 ssh_runner.go:235] Completed: sudo /var/lib/minikube/binaries/v1.30.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig: (1.715374831s)
I0903 20:56:39.928554    2781 kubeadm.go:1107] duration metric: took 1.715792773s to wait for elevateKubeSystemPrivileges
W0903 20:56:39.928634    2781 kubeadm.go:286] apiserver tunnel failed: apiserver port not set
I0903 20:56:39.928644    2781 kubeadm.go:393] duration metric: took 19.222459054s to StartCluster
I0903 20:56:39.928693    2781 settings.go:142] acquiring lock: {Name:mkde679bf3de7a23ad830a91c867919f42a2cbff Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:39.928894    2781 settings.go:150] Updating kubeconfig:  /home/ivan/.kube/config
I0903 20:56:39.935745    2781 lock.go:35] WriteFile acquiring /home/ivan/.kube/config: {Name:mkc919cfb1d2ec1b2b436d968595f19e2509875e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0903 20:56:39.936791    2781 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0903 20:56:39.936986    2781 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0903 20:56:39.937027    2781 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0903 20:56:39.938939    2781 out.go:177] 🔎  Verifying Kubernetes components...
I0903 20:56:39.941210    2781 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0903 20:56:39.938674    2781 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0903 20:56:39.941382    2781 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0903 20:56:39.941443    2781 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0903 20:56:39.941805    2781 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I0903 20:56:39.942066    2781 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0903 20:56:39.942557    2781 host.go:66] Checking if "minikube" exists ...
I0903 20:56:39.943374    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:39.943500    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:40.000483    2781 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0903 20:56:40.013317    2781 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0903 20:56:40.013536    2781 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0903 20:56:40.014022    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:40.089861    2781 addons.go:234] Setting addon default-storageclass=true in "minikube"
I0903 20:56:40.090025    2781 host.go:66] Checking if "minikube" exists ...
I0903 20:56:40.091728    2781 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0903 20:56:40.119868    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:40.181121    2781 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0903 20:56:40.181137    2781 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0903 20:56:40.181207    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0903 20:56:40.263403    2781 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:18508 SSHKeyPath:/home/ivan/.minikube/machines/minikube/id_rsa Username:docker}
I0903 20:56:40.424582    2781 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0903 20:56:40.487956    2781 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0903 20:56:40.900279    2781 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0903 20:56:40.921736    2781 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0903 20:56:41.796812    2781 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.30.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -": (1.372195917s)
I0903 20:56:41.796884    2781 start.go:946] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I0903 20:56:41.797420    2781 ssh_runner.go:235] Completed: sudo systemctl start kubelet: (1.308920374s)
I0903 20:56:41.797513    2781 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0903 20:56:41.861252    2781 api_server.go:52] waiting for apiserver process to appear ...
I0903 20:56:41.861651    2781 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0903 20:56:42.052244    2781 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.15193472s)
I0903 20:56:42.052352    2781 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.130591218s)
I0903 20:56:42.052731    2781 api_server.go:72] duration metric: took 2.115426521s to wait for apiserver process to appear ...
I0903 20:56:42.052739    2781 api_server.go:88] waiting for apiserver healthz status ...
I0903 20:56:42.052828    2781 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:18507/healthz ...
I0903 20:56:42.062317    2781 api_server.go:279] https://127.0.0.1:18507/healthz returned 200:
ok
I0903 20:56:42.063781    2781 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0903 20:56:42.065067    2781 addons.go:505] duration metric: took 2.12813846s for enable addons: enabled=[storage-provisioner default-storageclass]
I0903 20:56:42.064396    2781 api_server.go:141] control plane version: v1.30.0
I0903 20:56:42.065103    2781 api_server.go:131] duration metric: took 12.355694ms to wait for apiserver health ...
I0903 20:56:42.066020    2781 system_pods.go:43] waiting for kube-system pods to appear ...
I0903 20:56:42.078752    2781 system_pods.go:59] 5 kube-system pods found
I0903 20:56:42.078939    2781 system_pods.go:61] "etcd-minikube" [e5871758-7159-456e-bdd5-875c023c00b6] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0903 20:56:42.078947    2781 system_pods.go:61] "kube-apiserver-minikube" [9134c092-0d0f-4ec1-9fa9-e6e5eee8b410] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0903 20:56:42.078954    2781 system_pods.go:61] "kube-controller-manager-minikube" [ba6e71af-2c11-4b24-a1d8-c510f093a917] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0903 20:56:42.078961    2781 system_pods.go:61] "kube-scheduler-minikube" [4dc96f30-5fe5-4b3a-a371-075904397fe6] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0903 20:56:42.078964    2781 system_pods.go:61] "storage-provisioner" [198ab50b-6144-456b-8bde-d2a2e018a96f] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0903 20:56:42.078970    2781 system_pods.go:74] duration metric: took 12.938078ms to wait for pod list to return data ...
I0903 20:56:42.078979    2781 kubeadm.go:576] duration metric: took 2.141676267s to wait for: map[apiserver:true system_pods:true]
I0903 20:56:42.078990    2781 node_conditions.go:102] verifying NodePressure condition ...
I0903 20:56:42.083975    2781 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0903 20:56:42.084280    2781 node_conditions.go:123] node cpu capacity is 8
I0903 20:56:42.084844    2781 node_conditions.go:105] duration metric: took 5.615671ms to run NodePressure ...
I0903 20:56:42.084865    2781 start.go:240] waiting for startup goroutines ...
I0903 20:56:42.304056    2781 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0903 20:56:42.304108    2781 start.go:245] waiting for cluster config update ...
I0903 20:56:42.304169    2781 start.go:254] writing updated cluster config ...
I0903 20:56:42.304653    2781 ssh_runner.go:195] Run: rm -f paused
I0903 20:56:42.776308    2781 start.go:600] kubectl: 1.31.0, cluster: 1.30.0 (minor skew: 1)
I0903 20:56:42.778242    2781 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.773872492Z" level=info msg="Loading containers: start."
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.870912933Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.887985251Z" level=info msg="Processing signal 'terminated'"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.907473706Z" level=info msg="Loading containers: done."
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946809245Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946844721Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946851515Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946855839Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946876292Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.946921914Z" level=info msg="Daemon has completed initialization"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.984131322Z" level=info msg="API listen on /var/run/docker.sock"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.984140345Z" level=info msg="API listen on [::]:2376"
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.986061503Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Sep 03 12:56:16 minikube dockerd[1007]: time="2024-09-03T12:56:16.987184066Z" level=info msg="Daemon shutdown complete"
Sep 03 12:56:16 minikube systemd[1]: docker.service: Deactivated successfully.
Sep 03 12:56:16 minikube systemd[1]: Stopped Docker Application Container Engine.
Sep 03 12:56:16 minikube systemd[1]: Starting Docker Application Container Engine...
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.045799677Z" level=info msg="Starting up"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.068690395Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.112102328Z" level=info msg="Loading containers: start."
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.211820843Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.242710207Z" level=info msg="Loading containers: done."
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274713755Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274761602Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274771316Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274777170Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274796718Z" level=info msg="Docker daemon" commit=ac2de55 containerd-snapshotter=false storage-driver=overlay2 version=26.1.1
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.274847726Z" level=info msg="Daemon has completed initialization"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.312627528Z" level=info msg="API listen on /var/run/docker.sock"
Sep 03 12:56:17 minikube dockerd[1226]: time="2024-09-03T12:56:17.312629997Z" level=info msg="API listen on [::]:2376"
Sep 03 12:56:17 minikube systemd[1]: Started Docker Application Container Engine.
Sep 03 12:56:17 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Start docker client with request timeout 0s"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Hairpin mode is set to hairpin-veth"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Loaded network plugin cni"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Docker cri networking managed by network plugin cni"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Setting cgroupDriver cgroupfs"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Sep 03 12:56:17 minikube cri-dockerd[1452]: time="2024-09-03T12:56:17Z" level=info msg="Start cri-dockerd grpc backend"
Sep 03 12:56:17 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Sep 03 12:56:28 minikube cri-dockerd[1452]: time="2024-09-03T12:56:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b6a950ca51c18074ae150a60f2c6e956cb1d5b62b9b360631cad907c1d948a1c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:28 minikube cri-dockerd[1452]: time="2024-09-03T12:56:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d11bfdfca8ab22ecd7f8b796c007fc4b97674e0715fbb4d98d55b7b738731c98/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:28 minikube cri-dockerd[1452]: time="2024-09-03T12:56:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/72aeb848bbaaeb089489a4aa51386124eb458d520e811a3c63f3ef227070c529/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:28 minikube cri-dockerd[1452]: time="2024-09-03T12:56:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/43538c07aa16d4c321bf03beed78121bccb28fa0ed9e63413205bb8e036f6fc4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:52 minikube cri-dockerd[1452]: time="2024-09-03T12:56:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4662d6d9cbcd505f2003cdacd8b0208f6742ea63b3cb763aaa49f87924b3c5f4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:52 minikube cri-dockerd[1452]: time="2024-09-03T12:56:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb86564635f44ec5ea8dd356f6ae4f55bc5f3bb58ef7a4340a9750c2e4919c47/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:56:52 minikube cri-dockerd[1452]: time="2024-09-03T12:56:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9f708b51a3181887056e54c92c1d8baefc7a38904863a742d54fef92ef35ef36/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Sep 03 12:57:00 minikube cri-dockerd[1452]: time="2024-09-03T12:57:00Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Sep 03 12:57:13 minikube dockerd[1226]: time="2024-09-03T12:57:13.389234853Z" level=info msg="ignoring event" container=f655a66a95d63875503e11f6b67aa8aec7c70f57331b7f095d0612d036b0a452 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Sep 03 13:20:39 minikube cri-dockerd[1452]: time="2024-09-03T13:20:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d4354bfc5c0a483a6b05e81218f6f37103b5e4c225bdbecceb17191bca303a5/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 03 13:20:52 minikube cri-dockerd[1452]: time="2024-09-03T13:20:52Z" level=info msg="Pulling image mongo:latest: fee77be41765: Downloading [=============>                                     ]  59.07MB/226.6MB"
Sep 03 13:21:02 minikube cri-dockerd[1452]: time="2024-09-03T13:21:02Z" level=info msg="Pulling image mongo:latest: fee77be41765: Downloading [====================================>              ]  164.7MB/226.6MB"
Sep 03 13:21:12 minikube cri-dockerd[1452]: time="2024-09-03T13:21:12Z" level=info msg="Pulling image mongo:latest: fee77be41765: Extracting [====================>                              ]  93.03MB/226.6MB"
Sep 03 13:21:18 minikube cri-dockerd[1452]: time="2024-09-03T13:21:18Z" level=info msg="Stop pulling image mongo:latest: Status: Downloaded newer image for mongo:latest"
Sep 03 13:30:28 minikube cri-dockerd[1452]: time="2024-09-03T13:30:28Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a9b193194a2a6f1a7879c616300aff93c723034a89a45a2ea2b1fa2fc579467/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Sep 03 13:30:41 minikube cri-dockerd[1452]: time="2024-09-03T13:30:41Z" level=info msg="Pulling image mongo-express:latest: 9f7f59574f7d: Extracting [=============>                                     ]  3.768MB/13.64MB"
Sep 03 13:30:44 minikube cri-dockerd[1452]: time="2024-09-03T13:30:44Z" level=info msg="Stop pulling image mongo-express:latest: Status: Downloaded newer image for mongo-express:latest"


==> container status <==
CONTAINER           IMAGE                                                                                   CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
7f7e58c472c7c       mongo-express@sha256:1b23d7976f0210dbec74045c209e52fbb26d29b2e873d6c6fa3d3f0ae32c2a64   12 minutes ago      Running             mongo-express             0                   2a9b193194a2a       mongo-express-6cfbc86cb6-xjxrg
415e881d0b5bd       mongo@sha256:1a7b344b3ee8b07190fa15555726333e38f5db0a3bfb38b2ce9a1d3973b060be           21 minutes ago      Running             mongodb                   0                   9d4354bfc5c0a       mongodb-deployment-585bb4fddc-zl4zj
0bb1bd0adb273       6e38f40d628db                                                                           45 minutes ago      Running             storage-provisioner       1                   4662d6d9cbcd5       storage-provisioner
b5febdcedd013       cbb01a7bd410d                                                                           46 minutes ago      Running             coredns                   0                   9f708b51a3181       coredns-7db6d8ff4d-zktb5
577f25b19f7b2       a0bf559e280cf                                                                           46 minutes ago      Running             kube-proxy                0                   eb86564635f44       kube-proxy-t2zj6
f655a66a95d63       6e38f40d628db                                                                           46 minutes ago      Exited              storage-provisioner       0                   4662d6d9cbcd5       storage-provisioner
2bb0b493f3eb2       3861cfcd7c04c                                                                           46 minutes ago      Running             etcd                      0                   d11bfdfca8ab2       etcd-minikube
577e7f2810847       c42f13656d0b2                                                                           46 minutes ago      Running             kube-apiserver            0                   72aeb848bbaae       kube-apiserver-minikube
708f3f9cee438       c7aad43836fa5                                                                           46 minutes ago      Running             kube-controller-manager   0                   43538c07aa16d       kube-controller-manager-minikube
aba024e11312e       259c8277fcbbc                                                                           46 minutes ago      Running             kube-scheduler            0                   b6a950ca51c18       kube-scheduler-minikube


==> coredns [b5febdcedd01] <==
[INFO] 10.244.0.4:44340 - 29881 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000056987s
[INFO] 10.244.0.4:44340 - 29683 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000063659s
[INFO] 10.244.0.4:35108 - 27478 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000068825s
[INFO] 10.244.0.4:35108 - 27155 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000127602s
[INFO] 10.244.0.4:50290 - 52651 "AAAA IN mongo. udp 23 false 512" - - 0 2.001161218s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:53661->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:50290 - 52226 "A IN mongo. udp 23 false 512" - - 0 2.001117575s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:41092->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11266 "AAAA IN mongo. udp 23 false 512" - - 0 2.000655954s
[INFO] 10.244.0.4:52261 - 11109 "A IN mongo. udp 23 false 512" - - 0 2.000693342s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:41461->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:40755->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11109 "A IN mongo. udp 23 false 512" - - 0 2.00036407s
[INFO] 10.244.0.4:52261 - 11266 "AAAA IN mongo. udp 23 false 512" - - 0 2.000321552s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:36613->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:42118->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11109 "A IN mongo. udp 23 false 512" - - 0 2.001249285s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:43740->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11266 "AAAA IN mongo. udp 23 false 512" - - 0 2.001436908s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:59927->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11266 "AAAA IN mongo. udp 23 false 512" - - 0 2.000494893s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:41106->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11109 "A IN mongo. udp 23 false 512" - - 0 2.000494896s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:53716->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59755 - 27521 "AAAA IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000102742s
[INFO] 10.244.0.4:59755 - 27319 "A IN mongo.default.svc.cluster.local. udp 49 false 512" NXDOMAIN qr,aa,rd 142 0.000173577s
[INFO] 10.244.0.4:60713 - 7559 "AAAA IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000085447s
[INFO] 10.244.0.4:60713 - 7382 "A IN mongo.svc.cluster.local. udp 41 false 512" NXDOMAIN qr,aa,rd 134 0.000141498s
[INFO] 10.244.0.4:53328 - 23507 "AAAA IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000086233s
[INFO] 10.244.0.4:53328 - 23072 "A IN mongo.cluster.local. udp 37 false 512" NXDOMAIN qr,aa,rd 130 0.000116681s
[INFO] 10.244.0.4:52261 - 11266 "AAAA IN mongo. udp 23 false 512" - - 0 2.000429771s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:53971->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:52261 - 11109 "A IN mongo. udp 23 false 512" - - 0 2.000503569s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:49689->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37416 "AAAA IN mongo. udp 23 false 512" - - 0 2.001289173s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:43915->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37248 "A IN mongo. udp 23 false 512" - - 0 2.001274866s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:52806->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37248 "A IN mongo. udp 23 false 512" - - 0 2.001200472s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:59483->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37416 "AAAA IN mongo. udp 23 false 512" - - 0 2.001297302s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:54812->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37248 "A IN mongo. udp 23 false 512" - - 0 2.000605192s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:48266->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37416 "AAAA IN mongo. udp 23 false 512" - - 0 2.00060905s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:47878->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:49387 - 2406 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.00044758s
[INFO] 10.244.0.4:49387 - 2808 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000989358s
[INFO] 10.244.0.4:34844 - 57233 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000107133s
[INFO] 10.244.0.4:34844 - 57040 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000137094s
[INFO] 10.244.0.4:59758 - 37416 "AAAA IN mongo. udp 23 false 512" - - 0 2.000910073s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:34575->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37248 "A IN mongo. udp 23 false 512" - - 0 2.000906212s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:38145->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37248 "A IN mongo. udp 23 false 512" - - 0 2.0011227s
[ERROR] plugin/errors: 2 mongo. A: read udp 10.244.0.2:52266->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:59758 - 37416 "AAAA IN mongo. udp 23 false 512" - - 0 2.001130371s
[ERROR] plugin/errors: 2 mongo. AAAA: read udp 10.244.0.2:40657->192.168.65.254:53: i/o timeout
[INFO] 10.244.0.4:45391 - 26003 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000109024s
[INFO] 10.244.0.4:45391 - 26201 "AAAA IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.00016643s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=5883c09216182566a63dff4c326a6fc9ed2982ff
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_09_03T20_56_38_0700
                    minikube.k8s.io/version=v1.33.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 03 Sep 2024 12:56:33 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 03 Sep 2024 13:42:53 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 03 Sep 2024 13:41:23 +0000   Tue, 03 Sep 2024 12:56:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 03 Sep 2024 13:41:23 +0000   Tue, 03 Sep 2024 12:56:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 03 Sep 2024 13:41:23 +0000   Tue, 03 Sep 2024 12:56:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 03 Sep 2024 13:41:23 +0000   Tue, 03 Sep 2024 12:56:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8030240Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8030240Ki
  pods:               110
System Info:
  Machine ID:                 dac5fa0048bd4ff6a41bdf9f37e39220
  System UUID:                dac5fa0048bd4ff6a41bdf9f37e39220
  Boot ID:                    530b8146-e996-48c0-81da-adb493476394
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.1.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                   ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-6cfbc86cb6-xjxrg         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         12m
  default                     mongodb-deployment-585bb4fddc-zl4zj    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         22m
  kube-system                 coredns-7db6d8ff4d-zktb5               100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     46m
  kube-system                 etcd-minikube                          100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         46m
  kube-system                 kube-apiserver-minikube                250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         46m
  kube-system                 kube-controller-manager-minikube       200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         46m
  kube-system                 kube-proxy-t2zj6                       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         46m
  kube-system                 kube-scheduler-minikube                100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         46m
  kube-system                 storage-provisioner                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         46m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 46m                kube-proxy       
  Normal  NodeHasSufficientMemory  46m (x8 over 46m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    46m (x8 over 46m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     46m (x7 over 46m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  46m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  46m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    46m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     46m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 46m                kubelet          Starting kubelet.
  Normal  NodeNotReady             46m                kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeAllocatableEnforced  46m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeReady                46m                kubelet          Node minikube status is now: NodeReady
  Normal  RegisteredNode           46m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000230] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=0000000074f6e650
[  +0.000376] FS-Cache: N-key=[10] '34323934393337353137'
[  +0.152765] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007373] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000413] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000470] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000417] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.099412] systemd-journald[37]: File /var/log/journal/ca73d10959b1466cb6c7f8e488524345/system.journal corrupted or uncleanly shut down, renaming and replacing.
[Sep 3 12:52] FS-Cache: Duplicate cookie detected
[  +0.000390] FS-Cache: O-cookie c=00000028 [p=00000002 fl=222 nc=0 na=1]
[  +0.000320] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=000000008f3807a2
[  +0.000272] FS-Cache: O-key=[10] '34323934393734343435'
[  +0.000206] FS-Cache: N-cookie c=00000029 [p=00000002 fl=2 nc=0 na=1]
[  +0.000226] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=00000000ac0ff2a3
[  +0.000235] FS-Cache: N-key=[10] '34323934393734343435'
[  +0.022737] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.006515] FS-Cache: Duplicate cookie detected
[  +0.001819] FS-Cache: O-cookie c=0000002a [p=00000002 fl=222 nc=0 na=1]
[  +0.000232] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=00000000c3d69e97
[  +0.000256] FS-Cache: O-key=[10] '34323934393734343438'
[  +0.000179] FS-Cache: N-cookie c=0000002b [p=00000002 fl=2 nc=0 na=1]
[  +0.000288] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=0000000041b8e2c0
[  +0.000347] FS-Cache: N-key=[10] '34323934393734343438'
[  +0.002420] FS-Cache: Duplicate cookie detected
[  +0.000431] FS-Cache: O-cookie c=0000002a [p=00000002 fl=222 nc=0 na=1]
[  +0.000282] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=00000000c3d69e97
[  +0.000318] FS-Cache: O-key=[10] '34323934393734343438'
[  +0.000252] FS-Cache: N-cookie c=0000002c [p=00000002 fl=2 nc=0 na=1]
[  +0.000327] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=000000007e1a5d08
[  +0.000295] FS-Cache: N-key=[10] '34323934393734343438'
[  +0.018376] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Singapore not found. Is the tzdata package installed?
[  +0.041904] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.002449] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000380] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000596] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.181477] FS-Cache: Duplicate cookie detected
[  +0.000384] FS-Cache: O-cookie c=00000036 [p=00000002 fl=222 nc=0 na=1]
[  +0.000200] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=000000003ece948b
[  +0.000262] FS-Cache: O-key=[10] '34323934393734343733'
[  +0.000201] FS-Cache: N-cookie c=00000037 [p=00000002 fl=2 nc=0 na=1]
[  +0.000293] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=00000000bf7e0fcf
[  +0.000707] FS-Cache: N-key=[10] '34323934393734343733'
[  +0.233499] FS-Cache: Duplicate cookie detected
[  +0.000463] FS-Cache: O-cookie c=00000039 [p=00000002 fl=222 nc=0 na=1]
[  +0.000362] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=00000000ae23c63c
[  +0.000364] FS-Cache: O-key=[10] '34323934393734343937'
[  +0.000314] FS-Cache: N-cookie c=0000003a [p=00000002 fl=2 nc=0 na=1]
[  +0.000582] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=000000008040a4b3
[  +0.000557] FS-Cache: N-key=[10] '34323934393734343937'
[  +0.003647] FS-Cache: Duplicate cookie detected
[  +0.000494] FS-Cache: O-cookie c=00000039 [p=00000002 fl=222 nc=0 na=1]
[  +0.001486] FS-Cache: O-cookie d=00000000cb22ae79{9P.session} n=00000000ae23c63c
[  +0.000724] FS-Cache: O-key=[10] '34323934393734343937'
[  +0.000498] FS-Cache: N-cookie c=0000003b [p=00000002 fl=2 nc=0 na=1]
[  +0.000423] FS-Cache: N-cookie d=00000000cb22ae79{9P.session} n=000000002eaabfbd
[  +0.000477] FS-Cache: N-key=[10] '34323934393734343937'
[  +3.590169] new mount options do not match the existing superblock, will be ignored
[  +0.000227] netlink: 'init': attribute type 4 has an invalid length.


==> etcd [2bb0b493f3eb] <==
{"level":"info","ts":"2024-09-03T12:56:35.109715Z","caller":"traceutil/trace.go:171","msg":"trace[457467371] transaction","detail":"{read_only:false; response_revision:46; number_of_response:1; }","duration":"1.574553565s","start":"2024-09-03T12:56:33.535145Z","end":"2024-09-03T12:56:35.109699Z","steps":["trace[457467371] 'compare'  (duration: 1.568703943s)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:35.109788Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.535121Z","time spent":"1.574642059s","remote":"127.0.0.1:59106","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":622,"response count":0,"response size":38,"request content":"compare:<target:MOD key:\"/registry/flowschemas/probes\" mod_revision:0 > success:<request_put:<key:\"/registry/flowschemas/probes\" value_size:586 >> failure:<>"}
{"level":"warn","ts":"2024-09-03T12:56:35.112218Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.543570955s","expected-duration":"1s"}
{"level":"info","ts":"2024-09-03T12:56:35.112512Z","caller":"traceutil/trace.go:171","msg":"trace[697507849] transaction","detail":"{read_only:false; response_revision:47; number_of_response:1; }","duration":"1.573207159s","start":"2024-09-03T12:56:33.539289Z","end":"2024-09-03T12:56:35.112496Z","steps":["trace[697507849] 'process raft request'  (duration: 1.573105649s)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:35.112775Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.539238Z","time spent":"1.573333859s","remote":"127.0.0.1:59106","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1105,"response count":0,"response size":38,"request content":"compare:<target:MOD key:\"/registry/flowschemas/system-node-high\" mod_revision:44 > success:<request_put:<key:\"/registry/flowschemas/system-node-high\" value_size:1059 >> failure:<request_range:<key:\"/registry/flowschemas/system-node-high\" > >"}
{"level":"info","ts":"2024-09-03T12:56:35.114495Z","caller":"traceutil/trace.go:171","msg":"trace[410971745] linearizableReadLoop","detail":"{readStateIndex:52; appliedIndex:51; }","duration":"1.317681638s","start":"2024-09-03T12:56:33.796798Z","end":"2024-09-03T12:56:35.11448Z","steps":["trace[410971745] 'read index received'  (duration: 1.315825405s)","trace[410971745] 'applied index is now lower than readState.Index'  (duration: 1.855492ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-03T12:56:35.114543Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.544353Z","time spent":"1.570186104s","remote":"127.0.0.1:58762","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2024-09-03T12:56:35.115742Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.317788603s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/limitranges/kube-system/\" range_end:\"/registry/limitranges/kube-system0\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2024-09-03T12:56:35.115867Z","caller":"traceutil/trace.go:171","msg":"trace[582900687] range","detail":"{range_begin:/registry/limitranges/kube-system/; range_end:/registry/limitranges/kube-system0; response_count:0; response_revision:47; }","duration":"1.319085473s","start":"2024-09-03T12:56:33.796762Z","end":"2024-09-03T12:56:35.115848Z","steps":["trace[582900687] 'agreement among raft nodes before linearized reading'  (duration: 1.317791317s)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:35.115909Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.796746Z","time spent":"1.31914831s","remote":"127.0.0.1:58832","response type":"/etcdserverpb.KV/Range","request count":0,"request size":72,"response count":0,"response size":28,"request content":"key:\"/registry/limitranges/kube-system/\" range_end:\"/registry/limitranges/kube-system0\" "}
{"level":"warn","ts":"2024-09-03T12:56:35.117088Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"816.398941ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"warn","ts":"2024-09-03T12:56:35.117102Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.122053708s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2024-09-03T12:56:35.11714Z","caller":"traceutil/trace.go:171","msg":"trace[96065828] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:48; }","duration":"816.48609ms","start":"2024-09-03T12:56:34.300641Z","end":"2024-09-03T12:56:35.117127Z","steps":["trace[96065828] 'agreement among raft nodes before linearized reading'  (duration: 816.370123ms)"],"step_count":1}
{"level":"info","ts":"2024-09-03T12:56:35.117142Z","caller":"traceutil/trace.go:171","msg":"trace[1121689727] range","detail":"{range_begin:/registry/clusterroles/; range_end:/registry/clusterroles0; response_count:0; response_revision:48; }","duration":"1.122119353s","start":"2024-09-03T12:56:33.995012Z","end":"2024-09-03T12:56:35.117131Z","steps":["trace[1121689727] 'agreement among raft nodes before linearized reading'  (duration: 1.121977571s)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:35.117164Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:34.300622Z","time spent":"816.537082ms","remote":"127.0.0.1:58684","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":28,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2024-09-03T12:56:35.117167Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.995Z","time spent":"1.122161226s","remote":"127.0.0.1:59036","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":0,"response size":28,"request content":"key:\"/registry/clusterroles/\" range_end:\"/registry/clusterroles0\" "}
{"level":"warn","ts":"2024-09-03T12:56:35.117791Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.124047315s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses/system-node-critical\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2024-09-03T12:56:35.117829Z","caller":"traceutil/trace.go:171","msg":"trace[1891415260] range","detail":"{range_begin:/registry/priorityclasses/system-node-critical; range_end:; response_count:0; response_revision:48; }","duration":"1.124116327s","start":"2024-09-03T12:56:33.993704Z","end":"2024-09-03T12:56:35.11782Z","steps":["trace[1891415260] 'agreement among raft nodes before linearized reading'  (duration: 1.123242048s)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:35.117852Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:33.993688Z","time spent":"1.124158209s","remote":"127.0.0.1:59054","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":0,"response size":28,"request content":"key:\"/registry/priorityclasses/system-node-critical\" "}
{"level":"info","ts":"2024-09-03T12:56:35.219173Z","caller":"traceutil/trace.go:171","msg":"trace[1584197110] transaction","detail":"{read_only:false; response_revision:50; number_of_response:1; }","duration":"102.167157ms","start":"2024-09-03T12:56:35.116972Z","end":"2024-09-03T12:56:35.219139Z","steps":["trace[1584197110] 'process raft request'  (duration: 101.539677ms)"],"step_count":1}
{"level":"info","ts":"2024-09-03T12:56:35.219376Z","caller":"traceutil/trace.go:171","msg":"trace[578648280] linearizableReadLoop","detail":"{readStateIndex:55; appliedIndex:53; }","duration":"101.420071ms","start":"2024-09-03T12:56:35.117943Z","end":"2024-09-03T12:56:35.219363Z","steps":["trace[578648280] 'read index received'  (duration: 90.783037ms)","trace[578648280] 'applied index is now lower than readState.Index'  (duration: 10.636566ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-03T12:56:35.219462Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.505319ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2024-09-03T12:56:35.219492Z","caller":"traceutil/trace.go:171","msg":"trace[929963441] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:52; }","duration":"101.558204ms","start":"2024-09-03T12:56:35.117924Z","end":"2024-09-03T12:56:35.219483Z","steps":["trace[929963441] 'agreement among raft nodes before linearized reading'  (duration: 101.489204ms)"],"step_count":1}
{"level":"info","ts":"2024-09-03T12:56:38.989929Z","caller":"traceutil/trace.go:171","msg":"trace[1842630630] transaction","detail":"{read_only:false; response_revision:285; number_of_response:1; }","duration":"186.797243ms","start":"2024-09-03T12:56:38.803112Z","end":"2024-09-03T12:56:38.98991Z","steps":["trace[1842630630] 'process raft request'  (duration: 111.587784ms)","trace[1842630630] 'compare'  (duration: 74.748281ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-03T12:56:39.397859Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.515202ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128031646776649880 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/default/minikube.17f1bd3c49de9b26\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.17f1bd3c49de9b26\" value_size:570 lease:8128031646776649399 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2024-09-03T12:56:39.397944Z","caller":"traceutil/trace.go:171","msg":"trace[1175370347] linearizableReadLoop","detail":"{readStateIndex:295; appliedIndex:293; }","duration":"391.181549ms","start":"2024-09-03T12:56:39.00675Z","end":"2024-09-03T12:56:39.397932Z","steps":["trace[1175370347] 'read index received'  (duration: 78.353463ms)","trace[1175370347] 'applied index is now lower than readState.Index'  (duration: 312.827114ms)"],"step_count":2}
{"level":"info","ts":"2024-09-03T12:56:39.398013Z","caller":"traceutil/trace.go:171","msg":"trace[1765312295] transaction","detail":"{read_only:false; response_revision:288; number_of_response:1; }","duration":"391.652418ms","start":"2024-09-03T12:56:39.006353Z","end":"2024-09-03T12:56:39.398006Z","steps":["trace[1765312295] 'process raft request'  (duration: 235.938781ms)","trace[1765312295] 'compare'  (duration: 155.402431ms)"],"step_count":2}
{"level":"warn","ts":"2024-09-03T12:56:39.398057Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:39.006334Z","time spent":"391.703604ms","remote":"127.0.0.1:58762","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":638,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/minikube.17f1bd3c49de9b26\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/minikube.17f1bd3c49de9b26\" value_size:570 lease:8128031646776649399 >> failure:<>"}
{"level":"warn","ts":"2024-09-03T12:56:39.39832Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"391.563083ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:3695"}
{"level":"info","ts":"2024-09-03T12:56:39.398347Z","caller":"traceutil/trace.go:171","msg":"trace[1483636711] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:288; }","duration":"391.612917ms","start":"2024-09-03T12:56:39.006725Z","end":"2024-09-03T12:56:39.398338Z","steps":["trace[1483636711] 'agreement among raft nodes before linearized reading'  (duration: 391.530109ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:39.39837Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:39.006716Z","time spent":"391.648539ms","remote":"127.0.0.1:58876","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":1,"response size":3719,"request content":"key:\"/registry/minions/minikube\" "}
{"level":"warn","ts":"2024-09-03T12:56:39.398527Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"389.849783ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" ","response":"range_response_count:1 size:3695"}
{"level":"info","ts":"2024-09-03T12:56:39.398547Z","caller":"traceutil/trace.go:171","msg":"trace[1398999971] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:288; }","duration":"389.895087ms","start":"2024-09-03T12:56:39.008646Z","end":"2024-09-03T12:56:39.398541Z","steps":["trace[1398999971] 'agreement among raft nodes before linearized reading'  (duration: 389.855623ms)"],"step_count":1}
{"level":"warn","ts":"2024-09-03T12:56:39.398573Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2024-09-03T12:56:39.008634Z","time spent":"389.934993ms","remote":"127.0.0.1:58876","response type":"/etcdserverpb.KV/Range","request count":0,"request size":28,"response count":1,"response size":3719,"request content":"key:\"/registry/minions/minikube\" "}
{"level":"warn","ts":"2024-09-03T12:56:39.398729Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"275.588756ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/disruption-controller\" ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2024-09-03T12:56:39.398749Z","caller":"traceutil/trace.go:171","msg":"trace[1324039661] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/disruption-controller; range_end:; response_count:1; response_revision:288; }","duration":"275.635277ms","start":"2024-09-03T12:56:39.123105Z","end":"2024-09-03T12:56:39.39874Z","steps":["trace[1324039661] 'agreement among raft nodes before linearized reading'  (duration: 275.601147ms)"],"step_count":1}
{"level":"info","ts":"2024-09-03T13:06:30.950175Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":632}
{"level":"info","ts":"2024-09-03T13:06:30.95358Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":632,"took":"3.178447ms","hash":836859489,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1384448,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2024-09-03T13:06:30.95365Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":836859489,"revision":632,"compact-revision":-1}
{"level":"info","ts":"2024-09-03T13:11:30.944131Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":873}
{"level":"info","ts":"2024-09-03T13:11:30.945495Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":873,"took":"1.164495ms","hash":3115160974,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":897024,"current-db-size-in-use":"897 kB"}
{"level":"info","ts":"2024-09-03T13:11:30.945535Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3115160974,"revision":873,"compact-revision":632}
{"level":"info","ts":"2024-09-03T13:16:30.93622Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1112}
{"level":"info","ts":"2024-09-03T13:16:30.937485Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1112,"took":"1.053878ms","hash":1703482075,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":888832,"current-db-size-in-use":"889 kB"}
{"level":"info","ts":"2024-09-03T13:16:30.937523Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1703482075,"revision":1112,"compact-revision":873}
{"level":"info","ts":"2024-09-03T13:21:30.931009Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1353}
{"level":"info","ts":"2024-09-03T13:21:30.932933Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1353,"took":"1.66296ms","hash":3773241316,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":946176,"current-db-size-in-use":"946 kB"}
{"level":"info","ts":"2024-09-03T13:21:30.932996Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3773241316,"revision":1353,"compact-revision":1112}
{"level":"info","ts":"2024-09-03T13:26:30.923588Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1614}
{"level":"info","ts":"2024-09-03T13:26:30.925308Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1614,"took":"1.475975ms","hash":3311642802,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1036288,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-09-03T13:26:30.925367Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3311642802,"revision":1614,"compact-revision":1353}
{"level":"info","ts":"2024-09-03T13:31:30.916678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1859}
{"level":"info","ts":"2024-09-03T13:31:30.91838Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":1859,"took":"1.465737ms","hash":993430080,"current-db-size-bytes":1384448,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1146880,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-09-03T13:31:30.918434Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":993430080,"revision":1859,"compact-revision":1614}
{"level":"info","ts":"2024-09-03T13:36:30.908985Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2121}
{"level":"info","ts":"2024-09-03T13:36:30.911624Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2121,"took":"2.403099ms","hash":3238907393,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1208320,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2024-09-03T13:36:30.911663Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3238907393,"revision":2121,"compact-revision":1859}
{"level":"info","ts":"2024-09-03T13:41:30.902313Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2365}
{"level":"info","ts":"2024-09-03T13:41:30.904556Z","caller":"mvcc/kvstore_compaction.go:68","msg":"finished scheduled compaction","compact-revision":2365,"took":"1.922178ms","hash":2605029414,"current-db-size-bytes":1441792,"current-db-size":"1.4 MB","current-db-size-in-use-bytes":1097728,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-09-03T13:41:30.904611Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2605029414,"revision":2365,"compact-revision":2121}


==> kernel <==
 13:43:03 up 56 min,  0 users,  load average: 0.23, 0.25, 0.26
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [577e7f281084] <==
I0903 12:56:33.108702       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0903 12:56:33.108795       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0903 12:56:33.186763       1 shared_informer.go:320] Caches are synced for node_authorizer
I0903 12:56:33.187392       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0903 12:56:33.187576       1 policy_source.go:224] refreshing policies
E0903 12:56:33.189342       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0903 12:56:33.192615       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0903 12:56:33.192651       1 shared_informer.go:320] Caches are synced for configmaps
I0903 12:56:33.192832       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0903 12:56:33.192844       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0903 12:56:33.193256       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0903 12:56:33.193333       1 aggregator.go:165] initial CRD sync complete...
I0903 12:56:33.193342       1 autoregister_controller.go:141] Starting autoregister controller
I0903 12:56:33.193348       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0903 12:56:33.193353       1 cache.go:39] Caches are synced for autoregister controller
I0903 12:56:33.193865       1 controller.go:615] quota admission added evaluator for: namespaces
I0903 12:56:33.194888       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0903 12:56:33.415555       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0903 12:56:35.110452       1 trace.go:236] Trace[1319379532]: "Create" accept:application/vnd.kubernetes.protobuf, */*,audit-id:b9767021-a409-45df-94f8-8c67908a9bf7,client:::1,api-group:flowcontrol.apiserver.k8s.io,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:flowschemas,scope:resource,url:/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (03-Sep-2024 12:56:33.533) (total time: 1576ms):
Trace[1319379532]: ["Create etcd3" audit-id:b9767021-a409-45df-94f8-8c67908a9bf7,key:/flowschemas/probes,type:*flowcontrol.FlowSchema,resource:flowschemas.flowcontrol.apiserver.k8s.io 1576ms (12:56:33.534)
Trace[1319379532]:  ---"Txn call succeeded" 1575ms (12:56:35.110)]
Trace[1319379532]: [1.576673776s] [1.576673776s] END
I0903 12:56:35.113720       1 trace.go:236] Trace[1816810749]: "Patch" accept:application/vnd.kubernetes.protobuf, */*,audit-id:4b5b34da-fb71-4d80-a25c-e0458fb749a9,client:::1,api-group:flowcontrol.apiserver.k8s.io,api-version:v1,name:system-node-high,subresource:status,namespace:,protocol:HTTP/2.0,resource:flowschemas,scope:resource,url:/apis/flowcontrol.apiserver.k8s.io/v1/flowschemas/system-node-high/status,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:APPLY (03-Sep-2024 12:56:33.535) (total time: 1578ms):
Trace[1816810749]: ["GuaranteedUpdate etcd3" audit-id:4b5b34da-fb71-4d80-a25c-e0458fb749a9,key:/flowschemas/system-node-high,type:*flowcontrol.FlowSchema,resource:flowschemas.flowcontrol.apiserver.k8s.io 1577ms (12:56:33.535)
Trace[1816810749]:  ---"Txn call completed" 1576ms (12:56:35.113)]
Trace[1816810749]: ---"Object stored in database" 1576ms (12:56:35.113)
Trace[1816810749]: [1.578076457s] [1.578076457s] END
I0903 12:56:35.116480       1 trace.go:236] Trace[1959042382]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:bcf23660-8ece-4e04-808d-e83badcd5c41,client:::1,api-group:,api-version:v1,name:,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:limitranges,scope:namespace,url:/api/v1/namespaces/kube-system/limitranges,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:LIST (03-Sep-2024 12:56:33.796) (total time: 1320ms):
Trace[1959042382]: ["List(recursive=true) etcd3" audit-id:bcf23660-8ece-4e04-808d-e83badcd5c41,key:/limitranges/kube-system,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1320ms (12:56:33.796)]
Trace[1959042382]: [1.320251071s] [1.320251071s] END
I0903 12:56:35.117150       1 trace.go:236] Trace[1085913968]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9aed2bca-2a29-498b-b149-e3a9a5bb2d45,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:kube-system,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (03-Sep-2024 12:56:33.791) (total time: 1325ms):
Trace[1085913968]: ---"Write to database call failed" len:1205,err:pods "kube-scheduler-minikube" is forbidden: no PriorityClass with name system-node-critical was found 1323ms (12:56:35.117)
Trace[1085913968]: [1.325460277s] [1.325460277s] END
I0903 12:56:35.117655       1 trace.go:236] Trace[237345327]: "List" accept:application/vnd.kubernetes.protobuf, */*,audit-id:0cda0a64-c6f1-402e-8c17-4478d4aa0b85,client:::1,api-group:rbac.authorization.k8s.io,api-version:v1,name:,subresource:,namespace:,protocol:HTTP/2.0,resource:clusterroles,scope:cluster,url:/apis/rbac.authorization.k8s.io/v1/clusterroles,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:LIST (03-Sep-2024 12:56:33.992) (total time: 1124ms):
Trace[237345327]: ["List(recursive=true) etcd3" audit-id:0cda0a64-c6f1-402e-8c17-4478d4aa0b85,key:/clusterroles,resourceVersion:,resourceVersionMatch:,limit:0,continue: 1123ms (12:56:33.994)]
Trace[237345327]: [1.124812272s] [1.124812272s] END
I0903 12:56:35.118273       1 trace.go:236] Trace[1042136851]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f93c340d-99fa-40aa-8fe7-4a1ebfe07138,client:::1,api-group:scheduling.k8s.io,api-version:v1,name:system-node-critical,subresource:,namespace:,protocol:HTTP/2.0,resource:priorityclasses,scope:resource,url:/apis/scheduling.k8s.io/v1/priorityclasses/system-node-critical,user-agent:kube-apiserver/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:GET (03-Sep-2024 12:56:33.992) (total time: 1125ms):
Trace[1042136851]: [1.125460767s] [1.125460767s] END
I0903 12:56:35.209631       1 trace.go:236] Trace[1768953448]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ee0f4929-679c-4a0d-bef6-09838ec58549,client:192.168.49.2,api-group:,api-version:v1,name:,subresource:,namespace:default,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events,user-agent:kubelet/v1.30.0 (linux/amd64) kubernetes/7c48c2b,verb:POST (03-Sep-2024 12:56:33.491) (total time: 1718ms):
Trace[1768953448]: ["Create etcd3" audit-id:ee0f4929-679c-4a0d-bef6-09838ec58549,key:/events/default/minikube.17f1bd38f05a5c52,type:*core.Event,resource:events 1666ms (12:56:33.542)
Trace[1768953448]:  ---"TransformToStorage succeeded" 1571ms (12:56:35.115)
Trace[1768953448]:  ---"Txn call succeeded" 94ms (12:56:35.209)]
Trace[1768953448]: [1.71846479s] [1.71846479s] END
I0903 12:56:35.220769       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0903 12:56:35.227459       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0903 12:56:35.229034       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0903 12:56:36.064329       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0903 12:56:36.119233       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0903 12:56:36.207420       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0903 12:56:36.213904       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0903 12:56:36.216128       1 controller.go:615] quota admission added evaluator for: endpoints
I0903 12:56:36.221217       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0903 12:56:37.123491       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0903 12:56:37.948415       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0903 12:56:37.985339       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0903 12:56:38.026437       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0903 12:56:51.790728       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0903 12:56:51.818297       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0903 13:23:15.016010       1 alloc.go:330] "allocated clusterIPs" service="default/mongodb-service" clusterIPs={"IPv4":"10.102.135.55"}
I0903 13:32:39.248053       1 alloc.go:330] "allocated clusterIPs" service="default/mongo-express-service" clusterIPs={"IPv4":"10.99.0.183"}


==> kube-controller-manager [708f3f9cee43] <==
I0903 12:56:51.728210       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0903 12:56:51.730028       1 shared_informer.go:320] Caches are synced for GC
I0903 12:56:51.743895       1 shared_informer.go:320] Caches are synced for PV protection
I0903 12:56:51.756714       1 shared_informer.go:320] Caches are synced for daemon sets
I0903 12:56:51.759210       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0903 12:56:51.759222       1 shared_informer.go:320] Caches are synced for HPA
I0903 12:56:51.789803       1 shared_informer.go:320] Caches are synced for TTL
I0903 12:56:51.790780       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0903 12:56:51.791017       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0903 12:56:51.791256       1 shared_informer.go:320] Caches are synced for crt configmap
I0903 12:56:51.792888       1 shared_informer.go:320] Caches are synced for deployment
I0903 12:56:51.794881       1 shared_informer.go:320] Caches are synced for attach detach
I0903 12:56:51.798654       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0903 12:56:51.798986       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0903 12:56:51.804604       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0903 12:56:51.804757       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0903 12:56:51.804793       1 shared_informer.go:320] Caches are synced for node
I0903 12:56:51.804799       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0903 12:56:51.804821       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0903 12:56:51.804840       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0903 12:56:51.804844       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0903 12:56:51.804849       1 shared_informer.go:320] Caches are synced for cidrallocator
I0903 12:56:51.809569       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0903 12:56:51.813591       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0903 12:56:51.814677       1 shared_informer.go:320] Caches are synced for PVC protection
I0903 12:56:51.814830       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0903 12:56:51.815443       1 shared_informer.go:320] Caches are synced for disruption
I0903 12:56:51.815897       1 shared_informer.go:320] Caches are synced for stateful set
I0903 12:56:51.890665       1 range_allocator.go:381] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0903 12:56:51.904970       1 shared_informer.go:320] Caches are synced for job
I0903 12:56:51.907813       1 shared_informer.go:320] Caches are synced for cronjob
I0903 12:56:51.910696       1 shared_informer.go:320] Caches are synced for persistent volume
I0903 12:56:51.916866       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="25.763172ms"
I0903 12:56:51.923807       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="6.883794ms"
I0903 12:56:51.923934       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="73.563µs"
I0903 12:56:51.929616       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="224.898µs"
I0903 12:56:51.984695       1 shared_informer.go:320] Caches are synced for TTL after finished
I0903 12:56:51.993484       1 shared_informer.go:320] Caches are synced for endpoint
I0903 12:56:52.000368       1 shared_informer.go:320] Caches are synced for resource quota
I0903 12:56:52.007441       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0903 12:56:52.007594       1 shared_informer.go:320] Caches are synced for resource quota
I0903 12:56:52.421027       1 shared_informer.go:320] Caches are synced for garbage collector
I0903 12:56:52.505361       1 shared_informer.go:320] Caches are synced for garbage collector
I0903 12:56:52.505397       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0903 12:56:52.912001       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="194.147µs"
I0903 12:56:53.960700       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="15.751369ms"
I0903 12:56:53.960904       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="113.091µs"
I0903 13:20:38.681459       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="9.520643ms"
I0903 13:20:38.690759       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="9.224002ms"
I0903 13:20:38.690825       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="32.698µs"
I0903 13:20:38.690851       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="14.16µs"
I0903 13:20:38.690890       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="22.538µs"
I0903 13:21:20.007935       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="5.108435ms"
I0903 13:21:20.008037       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongodb-deployment-585bb4fddc" duration="35.464µs"
I0903 13:30:28.307944       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="10.343518ms"
I0903 13:30:28.315351       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="7.335445ms"
I0903 13:30:28.315434       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="47.599µs"
I0903 13:30:28.320647       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="29.873µs"
I0903 13:30:46.881853       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="4.231445ms"
I0903 13:30:46.881936       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/mongo-express-6cfbc86cb6" duration="30.651µs"


==> kube-proxy [577f25b19f7b] <==
I0903 12:56:52.521414       1 server_linux.go:69] "Using iptables proxy"
I0903 12:56:52.535608       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0903 12:56:52.553244       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0903 12:56:52.553301       1 server_linux.go:165] "Using iptables Proxier"
I0903 12:56:52.585353       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0903 12:56:52.585392       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0903 12:56:52.586326       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0903 12:56:52.587168       1 server.go:872] "Version info" version="v1.30.0"
I0903 12:56:52.587233       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0903 12:56:52.590763       1 config.go:192] "Starting service config controller"
I0903 12:56:52.590860       1 config.go:101] "Starting endpoint slice config controller"
I0903 12:56:52.590961       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0903 12:56:52.590961       1 shared_informer.go:313] Waiting for caches to sync for service config
I0903 12:56:52.591106       1 config.go:319] "Starting node config controller"
I0903 12:56:52.591115       1 shared_informer.go:313] Waiting for caches to sync for node config
I0903 12:56:52.691465       1 shared_informer.go:320] Caches are synced for service config
I0903 12:56:52.691513       1 shared_informer.go:320] Caches are synced for node config
I0903 12:56:52.691486       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [aba024e11312] <==
E0903 12:56:33.195056       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:33.195061       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0903 12:56:33.195075       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0903 12:56:33.203354       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0903 12:56:33.203396       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0903 12:56:33.203405       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0903 12:56:33.203418       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0903 12:56:33.203437       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0903 12:56:33.203471       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0903 12:56:33.205536       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0903 12:56:33.205571       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0903 12:56:33.205790       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0903 12:56:33.205818       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0903 12:56:33.206121       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:33.206165       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:33.206295       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0903 12:56:33.206338       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0903 12:56:33.206772       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:33.206817       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:33.206834       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0903 12:56:33.206854       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0903 12:56:33.206861       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0903 12:56:33.206867       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0903 12:56:33.207680       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0903 12:56:33.207725       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0903 12:56:33.207923       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0903 12:56:33.207946       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0903 12:56:34.002463       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:34.002526       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:34.024154       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:34.024219       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:34.086513       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0903 12:56:34.086577       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0903 12:56:34.175508       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0903 12:56:34.175573       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0903 12:56:34.267373       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0903 12:56:34.267447       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0903 12:56:34.267825       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0903 12:56:34.267873       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0903 12:56:34.299219       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:34.299286       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:34.321250       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:34.321312       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0903 12:56:34.341348       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0903 12:56:34.341420       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0903 12:56:34.358518       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0903 12:56:34.358587       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0903 12:56:34.399469       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0903 12:56:34.399524       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0903 12:56:34.568260       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0903 12:56:34.568318       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0903 12:56:34.629287       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0903 12:56:34.629349       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0903 12:56:34.653956       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0903 12:56:34.654068       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0903 12:56:34.654210       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0903 12:56:34.654224       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0903 12:56:35.849938       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0903 12:56:35.850004       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
I0903 12:56:36.332571       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.888521    2384 cpu_manager.go:214] "Starting CPU manager" policy="none"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.888550    2384 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.888629    2384 state_mem.go:36] "Initialized new in-memory state store"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.889435    2384 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.889567    2384 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.889824    2384 policy_none.go:49] "None policy: Start"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.895917    2384 memory_manager.go:170] "Starting memorymanager" policy="None"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.896193    2384 state_mem.go:35] "Initializing new in-memory state store"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.896975    2384 state_mem.go:75] "Updated machine memory state"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.899373    2384 manager.go:479] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.900141    2384 container_log_manager.go:186] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Sep 03 12:56:39 minikube kubelet[2384]: I0903 12:56:39.902090    2384 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.093332    2384 topology_manager.go:215] "Topology Admit Handler" podUID="7fd44e8d11c3e0ffe6b1825e2a1f2270" podNamespace="kube-system" podName="kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.094169    2384 topology_manager.go:215] "Topology Admit Handler" podUID="f9c8e1d0d74b1727abdb4b4a31d3a7c1" podNamespace="kube-system" podName="kube-scheduler-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.094818    2384 topology_manager.go:215] "Topology Admit Handler" podUID="063d6b9688927e601f52fd818d1305c5" podNamespace="kube-system" podName="etcd-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.095385    2384 topology_manager.go:215] "Topology Admit Handler" podUID="3c555f828409b009ebee39fdbedfcac0" podNamespace="kube-system" podName="kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.185790    2384 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218236    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218348    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218384    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-data\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218407    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218431    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218456    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218478    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/f9c8e1d0d74b1727abdb4b4a31d3a7c1-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"f9c8e1d0d74b1727abdb4b4a31d3a7c1\") " pod="kube-system/kube-scheduler-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218497    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/063d6b9688927e601f52fd818d1305c5-etcd-certs\") pod \"etcd-minikube\" (UID: \"063d6b9688927e601f52fd818d1305c5\") " pod="kube-system/etcd-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218519    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218538    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218558    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218579    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218601    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218620    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/7fd44e8d11c3e0ffe6b1825e2a1f2270-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"7fd44e8d11c3e0ffe6b1825e2a1f2270\") " pod="kube-system/kube-controller-manager-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.218655    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3c555f828409b009ebee39fdbedfcac0-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"3c555f828409b009ebee39fdbedfcac0\") " pod="kube-system/kube-apiserver-minikube"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.899070    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=0.898800271 podStartE2EDuration="898.800271ms" podCreationTimestamp="2024-09-03 12:56:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:40.887257314 +0000 UTC m=+2.953836938" watchObservedRunningTime="2024-09-03 12:56:40.898800271 +0000 UTC m=+2.965379886"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.917958    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=0.917870716 podStartE2EDuration="917.870716ms" podCreationTimestamp="2024-09-03 12:56:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:40.899961003 +0000 UTC m=+2.966540653" watchObservedRunningTime="2024-09-03 12:56:40.917870716 +0000 UTC m=+2.984450348"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.986443    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=0.98641045 podStartE2EDuration="986.41045ms" podCreationTimestamp="2024-09-03 12:56:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:40.986056073 +0000 UTC m=+3.052635694" watchObservedRunningTime="2024-09-03 12:56:40.98641045 +0000 UTC m=+3.052990073"
Sep 03 12:56:40 minikube kubelet[2384]: I0903 12:56:40.986571    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=0.98656554 podStartE2EDuration="986.56554ms" podCreationTimestamp="2024-09-03 12:56:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:40.918515153 +0000 UTC m=+2.985094775" watchObservedRunningTime="2024-09-03 12:56:40.98656554 +0000 UTC m=+3.053145165"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.741777    2384 topology_manager.go:215] "Topology Admit Handler" podUID="198ab50b-6144-456b-8bde-d2a2e018a96f" podNamespace="kube-system" podName="storage-provisioner"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.817715    2384 topology_manager.go:215] "Topology Admit Handler" podUID="d117629e-8398-4495-8357-5e0e8cf2fcec" podNamespace="kube-system" podName="kube-proxy-t2zj6"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849632    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/d117629e-8398-4495-8357-5e0e8cf2fcec-lib-modules\") pod \"kube-proxy-t2zj6\" (UID: \"d117629e-8398-4495-8357-5e0e8cf2fcec\") " pod="kube-system/kube-proxy-t2zj6"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849676    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-t2jmv\" (UniqueName: \"kubernetes.io/projected/d117629e-8398-4495-8357-5e0e8cf2fcec-kube-api-access-t2jmv\") pod \"kube-proxy-t2zj6\" (UID: \"d117629e-8398-4495-8357-5e0e8cf2fcec\") " pod="kube-system/kube-proxy-t2zj6"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849692    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/198ab50b-6144-456b-8bde-d2a2e018a96f-tmp\") pod \"storage-provisioner\" (UID: \"198ab50b-6144-456b-8bde-d2a2e018a96f\") " pod="kube-system/storage-provisioner"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849713    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/d117629e-8398-4495-8357-5e0e8cf2fcec-kube-proxy\") pod \"kube-proxy-t2zj6\" (UID: \"d117629e-8398-4495-8357-5e0e8cf2fcec\") " pod="kube-system/kube-proxy-t2zj6"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849722    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/d117629e-8398-4495-8357-5e0e8cf2fcec-xtables-lock\") pod \"kube-proxy-t2zj6\" (UID: \"d117629e-8398-4495-8357-5e0e8cf2fcec\") " pod="kube-system/kube-proxy-t2zj6"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.849732    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fkjsd\" (UniqueName: \"kubernetes.io/projected/198ab50b-6144-456b-8bde-d2a2e018a96f-kube-api-access-fkjsd\") pod \"storage-provisioner\" (UID: \"198ab50b-6144-456b-8bde-d2a2e018a96f\") " pod="kube-system/storage-provisioner"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.915964    2384 topology_manager.go:215] "Topology Admit Handler" podUID="d9f55dd0-7afc-4311-88e9-fd04e5509d05" podNamespace="kube-system" podName="coredns-7db6d8ff4d-zktb5"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.950900    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/d9f55dd0-7afc-4311-88e9-fd04e5509d05-config-volume\") pod \"coredns-7db6d8ff4d-zktb5\" (UID: \"d9f55dd0-7afc-4311-88e9-fd04e5509d05\") " pod="kube-system/coredns-7db6d8ff4d-zktb5"
Sep 03 12:56:51 minikube kubelet[2384]: I0903 12:56:51.950956    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-scvnr\" (UniqueName: \"kubernetes.io/projected/d9f55dd0-7afc-4311-88e9-fd04e5509d05-kube-api-access-scvnr\") pod \"coredns-7db6d8ff4d-zktb5\" (UID: \"d9f55dd0-7afc-4311-88e9-fd04e5509d05\") " pod="kube-system/coredns-7db6d8ff4d-zktb5"
Sep 03 12:56:52 minikube kubelet[2384]: I0903 12:56:52.915511    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-7db6d8ff4d-zktb5" podStartSLOduration=1.9154932169999999 podStartE2EDuration="1.915493217s" podCreationTimestamp="2024-09-03 12:56:51 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:52.915339122 +0000 UTC m=+14.981918736" watchObservedRunningTime="2024-09-03 12:56:52.915493217 +0000 UTC m=+14.982072832"
Sep 03 12:56:52 minikube kubelet[2384]: I0903 12:56:52.926550    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-t2zj6" podStartSLOduration=1.926532061 podStartE2EDuration="1.926532061s" podCreationTimestamp="2024-09-03 12:56:51 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:52.926376758 +0000 UTC m=+14.992956373" watchObservedRunningTime="2024-09-03 12:56:52.926532061 +0000 UTC m=+14.993111671"
Sep 03 12:56:52 minikube kubelet[2384]: I0903 12:56:52.935088    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=10.935068502 podStartE2EDuration="10.935068502s" podCreationTimestamp="2024-09-03 12:56:42 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-09-03 12:56:52.935045033 +0000 UTC m=+15.001624645" watchObservedRunningTime="2024-09-03 12:56:52.935068502 +0000 UTC m=+15.001648132"
Sep 03 12:57:00 minikube kubelet[2384]: I0903 12:57:00.194075    2384 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Sep 03 12:57:00 minikube kubelet[2384]: I0903 12:57:00.195652    2384 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Sep 03 12:57:14 minikube kubelet[2384]: I0903 12:57:14.020322    2384 scope.go:117] "RemoveContainer" containerID="f655a66a95d63875503e11f6b67aa8aec7c70f57331b7f095d0612d036b0a452"
Sep 03 13:20:38 minikube kubelet[2384]: I0903 13:20:38.682332    2384 topology_manager.go:215] "Topology Admit Handler" podUID="1572fa30-bc3a-4986-b0f9-98e8830f6b0f" podNamespace="default" podName="mongodb-deployment-585bb4fddc-zl4zj"
Sep 03 13:20:38 minikube kubelet[2384]: I0903 13:20:38.715367    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-gbcwl\" (UniqueName: \"kubernetes.io/projected/1572fa30-bc3a-4986-b0f9-98e8830f6b0f-kube-api-access-gbcwl\") pod \"mongodb-deployment-585bb4fddc-zl4zj\" (UID: \"1572fa30-bc3a-4986-b0f9-98e8830f6b0f\") " pod="default/mongodb-deployment-585bb4fddc-zl4zj"
Sep 03 13:30:28 minikube kubelet[2384]: I0903 13:30:28.309058    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongodb-deployment-585bb4fddc-zl4zj" podStartSLOduration=550.860747241 podStartE2EDuration="9m50.308960645s" podCreationTimestamp="2024-09-03 13:20:38 +0000 UTC" firstStartedPulling="2024-09-03 13:20:39.159951287 +0000 UTC m=+1441.282389904" lastFinishedPulling="2024-09-03 13:21:18.607223129 +0000 UTC m=+1480.730603308" observedRunningTime="2024-09-03 13:21:20.003959909 +0000 UTC m=+1482.127340111" watchObservedRunningTime="2024-09-03 13:30:28.308960645 +0000 UTC m=+2030.453770622"
Sep 03 13:30:28 minikube kubelet[2384]: I0903 13:30:28.309468    2384 topology_manager.go:215] "Topology Admit Handler" podUID="f240c2e4-7211-426e-bf98-8b76059d0b1f" podNamespace="default" podName="mongo-express-6cfbc86cb6-xjxrg"
Sep 03 13:30:28 minikube kubelet[2384]: I0903 13:30:28.335079    2384 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q242s\" (UniqueName: \"kubernetes.io/projected/f240c2e4-7211-426e-bf98-8b76059d0b1f-kube-api-access-q242s\") pod \"mongo-express-6cfbc86cb6-xjxrg\" (UID: \"f240c2e4-7211-426e-bf98-8b76059d0b1f\") " pod="default/mongo-express-6cfbc86cb6-xjxrg"
Sep 03 13:30:28 minikube kubelet[2384]: I0903 13:30:28.757275    2384 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2a9b193194a2a6f1a7879c616300aff93c723034a89a45a2ea2b1fa2fc579467"
Sep 03 13:30:46 minikube kubelet[2384]: I0903 13:30:46.877808    2384 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongo-express-6cfbc86cb6-xjxrg" podStartSLOduration=3.229558402 podStartE2EDuration="18.877792283s" podCreationTimestamp="2024-09-03 13:30:28 +0000 UTC" firstStartedPulling="2024-09-03 13:30:28.789520176 +0000 UTC m=+2030.934330137" lastFinishedPulling="2024-09-03 13:30:44.436576936 +0000 UTC m=+2046.582564018" observedRunningTime="2024-09-03 13:30:46.877480696 +0000 UTC m=+2049.023467784" watchObservedRunningTime="2024-09-03 13:30:46.877792283 +0000 UTC m=+2049.023779364"


==> storage-provisioner [0bb1bd0adb27] <==
I0903 12:57:14.121855       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0903 12:57:14.129771       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0903 12:57:14.129836       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0903 12:57:14.138423       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0903 12:57:14.138506       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5625091b-d609-4581-9d35-d5f833b0004b", APIVersion:"v1", ResourceVersion:"426", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_c2beb047-9921-4b40-827e-ca930c78786d became leader
I0903 12:57:14.138597       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_c2beb047-9921-4b40-827e-ca930c78786d!
I0903 12:57:14.239041       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_c2beb047-9921-4b40-827e-ca930c78786d!


==> storage-provisioner [f655a66a95d6] <==
I0903 12:56:52.332386       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0903 12:57:13.370920       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

